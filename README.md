# killmeLLM
Building a LLM using Distributed Systems

Assignment 1 Project Documentation: CS441 Engineering Distributed Objects for Cloud Computing


Input Data:
Command to run the code
  sbt clean compile test
  sbt clean compile run.

I used IMDB reviews for this project, consisting of positive and negative feedback. The dataset contains 10,000 reviews. The input data was split into five text shards, though tests were conducted with 3, 7, and 10 shards. The configuration using 5 shards provided the optimal performance.
MapReduce Pipeline:
We have implemented three MapReduce jobs to accomplish different tasks as part of the data processing pipeline.
	1.	Tokenization:
	•	Description: Implements byte pair encoding (BPE) for tokenizing the input reviews.
	•	Input:
	•	Key: LongWritable (the byte offset of the line in the text file).
	•	Value: Text (the actual review content).
	•	Output:
	•	Key: Text (a tokenized word).
	•	Value: Text (the count “1” or any other associated value for the token).
	•	Purpose: Prepares text for downstream tasks by converting it into tokenized words.
	2.	Word2Vec Embedding Generation:
	•	Description: This task uses the Word2Vec model to generate vector embeddings for each tokenized sentence.
	•	Input: A list of tokenized sentences.
	•	Output: Vector embeddings for each word. Each word’s vector has a size of 10 dimensions.
	•	Purpose: Converts words into numerical vectors to facilitate calculations such as similarity measures.
	3.	Cosine Similarity Calculation:
	•	Description: The final MapReduce task calculates the cosine similarity between vector embeddings generated by Word2Vec. Based on the calculated cosine similarity, it classifies the word pairs into one of the following categories:
	•	A: 0.00 - 0.25
	•	B: 0.25 - 0.50
	•	C: 0.50 - 0.75
	•	D: 0.75 - 1.00
	•	Purpose: This step helps group words based on their semantic similarity using the cosine of their vector angle.

Performance Insights:
	•	Shard Count Testing: Tests were run with 3, 7, and 10 shards to optimize performance. We found that 5 shards provided the most balanced results, with optimal efficiency and resource utilization.
This pipeline effectively processes large textual datasets, tokenizes the data, and generates meaningful vector representations that are classified based on similarity.

Output:
The output.csv contains the Frequency of the token and word.
The Output directory has Output from the tokenization./
The Word2Vec directory has output from the Word2Vec.
The cosine-similarity directory has output from the cosine similarity.



  


